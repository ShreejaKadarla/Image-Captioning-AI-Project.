# ðŸ§  Image Captioning using BLIP

This project demonstrates an AI-based image captioning system using a pre-trained **BLIP (Bootstrapping Language-Image Pretraining)** model. It generates natural language descriptions for images, making it useful for accessibility, content tagging, and more.

---

## ðŸŽ¯ Project Objective

The objective of this project is to demonstrate how pre-trained vision-language models can automatically describe visual content. This technology has real-world applications in:
- Accessibility for visually impaired users
- Smart photo organization
- Content search and tagging
- Automated image moderation

---

## ðŸ“Œ Project Overview

- **Model Used**: Salesforce BLIP (Base version)  
- **Library**: Hugging Face Transformers  
- **Framework**: PyTorch  
- **Functionality**: Automatically generates a caption describing the input image.

---

## ðŸ§  How It Works

1. Load a pre-trained BLIP model and processor from Hugging Face.
2. Load the input image from a URL or local path.
3. Preprocess the image using the processor.
4. Generate a caption using the BLIP model.
5. Decode the modelâ€™s output into human-readable text.

---




